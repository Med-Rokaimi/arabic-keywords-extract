{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting Arabic keywords from social media posts"
      ],
      "metadata": {
        "id": "oKZvksAVwfZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides two examples of how to extract Arabic keywords from social media posts using the Python KeyBERT package. KeyBERT is a simple and easy-to-use keyword extraction technique that uses BERT embeddings to generate keywords and keyphrases that are most similar to a document. However, extracting keywords from social media posts is not as straightforward as extracting from documents, and existing tools do not work as well with Arabic as they do with English.\n",
        "\n"
      ],
      "metadata": {
        "id": "u6aOZSD5zLvT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wI4JR8xwZR6"
      },
      "outputs": [],
      "source": [
        "# install  packages\n",
        "\n",
        "!pip install keybert\n",
        "!pip install keyphrase-vectorizers\n",
        "!pip install pyarabic\n",
        "!pip install Arabic-stopwords\n",
        "!pip install camel-tools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing"
      ],
      "metadata": {
        "id": "wfjwRAHF0Qg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pyarabic.araby import tokenize, is_arabicrange, strip_tashkeel\n",
        "import arabicstopwords.arabicstopwords as stpw\n",
        "\n",
        "\n",
        "def text_processing(text):\n",
        "  # remove non arabic data\n",
        "  #text=tokenize(text, conditions=is_arabicrange, morphs=strip_tashkeel)\n",
        "  text=tokenize(text,  morphs=strip_tashkeel)\n",
        "\n",
        "  #remove arabic stopwords\n",
        "  stp_words_list = open(\"./arabicstopword.txt\", encoding=\"utf-8\")\n",
        "  stp_words = stp_words_list.read()\n",
        "  stp_words = stp_words.split(\"\\n\")\n",
        "  from camel_tools.utils.normalize import(\n",
        "    normalize_alef_ar,\n",
        "    normalize_alef_maksura_ar,\n",
        "    normalize_teh_marbuta_ar,\n",
        "    )\n",
        "  added = []\n",
        "\n",
        "  for word in stp_words:\n",
        "    nar = normalize_alef_ar(word)\n",
        "    nam = normalize_alef_maksura_ar(nar)\n",
        "    res = normalize_teh_marbuta_ar(word)\n",
        "    added.append(res)\n",
        "\n",
        "  stp_words.append(added)\n",
        "  tokens = [w for w in text if not stpw.is_stop(w)]\n",
        "  tokens = [word for word in text if not word in stp_words]\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "2ya1NiEXxDuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm going to implement the extraction function with KeyBERT using two different classes:\n",
        "\n",
        "1.   CountVectorizer\n",
        "2.   KeyphraseCountVectorizer\n"
      ],
      "metadata": {
        "id": "EvdV-KMjxk1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1- using CounterVectorize"
      ],
      "metadata": {
        "id": "9-iJaeyDyx5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default model in KeyBERT (\"all-MiniLM-L6-v2\") works great for English documents. In contrast, for multi-lingual documents or any other language, \"paraphrase-multilingual-MiniLM-L12-v2\"\" has shown better performance.\n",
        "However, using (paraphrase-mpnet-base-v2) and paraphrase-multilingual-mpnet-base-v2 perofrm well but with higher complixity.\n",
        "\n"
      ],
      "metadata": {
        "id": "f7-IACt1yIPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#==================================================\n",
        "#1- using CounterVectorize\n",
        "#==================================================\n",
        "\n",
        "def extract_non_verb_keywords(sentence):\n",
        "  from transformers import pipeline\n",
        "  pos = pipeline('token-classification', model='CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-msa')\n",
        "  pos=pos(sentence)\n",
        "  non_verb_kw=[]\n",
        "  for i in range (len(pos)):\n",
        "    kw=[]\n",
        "    for element in pos[i]:\n",
        "      if element.get('entity') not in ['verb', 'prep']:\n",
        "       w = element.get('word')\n",
        "       kw.append(w)\n",
        "       listToStr = ' '.join([str(elem) for elem in kw])\n",
        "       listToStr=listToStr.replace(\" ##\",\"\")\n",
        "    non_verb_kw.append(listToStr)\n",
        "  return non_verb_kw"
      ],
      "metadata": {
        "id": "VbAJ_IiWzDbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def arabic_keyword_extract_CV(text):\n",
        "  from keybert import KeyBERT\n",
        "  from sklearn.feature_extraction.text import CountVectorizer\n",
        "  #processing data\n",
        "  tokens=text_processing(text5)\n",
        "  text=\" \".join(tokens)\n",
        "  # with CountVectorizer, we should to define manually the keyword length using ngram_range\n",
        "  vectorizer= CountVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "  kw_extractor = KeyBERT('paraphrase-multilingual-mpnet-base-v2')\n",
        "  keywords = kw_extractor.extract_keywords(text , vectorizer=vectorizer, stop_words=\"arabic\", use_mmr=True, diversity=0.7, top_n=5)\n",
        "  keywords=[item[0] for item in keywords]\n",
        "\n",
        "  #remove verbs\n",
        "  non_verb_keywords= extract_non_verb_keywords(keywords)\n",
        "  import re\n",
        "  # Define a regular expression pattern for Arabic letters\n",
        "  keywods= [keword for keword in non_verb_keywords if re.search('[\\u0600-\\u06FF\\u0750-\\u077F]+', keword)]\n",
        "  output = dict()\n",
        "  output[\"original_text\"] = text\n",
        "  output[\"keywords\"] = [','.join(keywods)]\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(output)\n",
        "  df.to_csv(\"arabic_keywords_CV.csv\")\n",
        "  return output\n",
        "\n",
        "#testing\n",
        "text=\"Ù„ØªÙƒÙ† Ø¹Ø¨Ø±Ø© Ù„Ù„Ø§Ø®Ø±ÙŠÙ† Ø§ØµØ¨Ø­Ù†Ø§ Ù†Ø±Ù‰ Ø§Ù†ØªØ´Ø§Ø± ÙˆØ§Ø³Ø¹ Ù„Ù„ØªØ§ÙÙ‡ÙŠÙ† Ø¨Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆØ§ØµØ¨Ø­Ùˆ Ø­Ø¯ÙŠØ« Ø§Ù„ÙƒÙ„ Ø±ØºÙ… Ø§Ù†Ù‡Ù… Ù„Ù… ÙŠÙ‚Ø¯Ù…Ùˆ Ø§ÙŠ Ù‚ÙŠÙ…Ø© Ù…Ø¶Ø§ÙØ© Ù„Ù„Ù…Ø¬ØªÙ…Ø¹ Ù„Ø§ÙƒÙ† Ù„Ù„Ø§Ø³Ù ÙØ§Ù† Ø§Ù„Ù†Ø¬ÙˆÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙŠØ³ØªØ­Ù‚ÙˆÙ† Ø§Ù† ÙŠØªØµØ¯Ø±Ùˆ Ø§Ù„Ø·ÙˆÙ†Ø¯ÙˆÙ†Ø³ Ù†ÙƒØ§Ø¯ Ù„Ø§Ù†Ø³Ù…Ø¹ Ø¹Ù†Ù‡Ù… Ø´ÙŠØ¡ Ø±ØºÙ… Ø§Ù†Ù‡Ù… ÙŠØ­ØªÙ„ÙˆÙ†Ø© Ù…ÙƒØ§Ù†Ø© Ø±ÙÙŠØ¹Ø© ÙÙŠ Ø§ÙƒØ¨Ø± Ø§Ù„Ù…Ø¤Ø³Ø³Ø§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø§Ø±ÙˆØ¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© .,ar\"\n",
        "text2=\" Ø£Ø®ÙŠ Ø§Ù„ÙÙ„Ø§Ø­ØŒ Ø£Ø®ØªÙŠ Ø§Ù„ÙÙ„Ø§Ø­Ø©ØŒ Ø§Ù„Ù‚Ø±Ø¶ Ø§Ù„ÙÙ„Ø§Ø­ÙŠ Ù„Ù„Ù…ØºØ±Ø¨ ÙƒÙŠÙ†Ø®Ø§Ø±Ø· Ø¨Ø¬Ù†Ø¨ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø±Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ù„Ù„ØªØ£Ù…ÙŠÙ† Ø§Ù„Ø§Ø¬Ø¨Ø§Ø±ÙŠ Ø¹Ù† Ø§Ù„Ù…Ø±Ø¶ Ù„Ø¶Ù…Ø§Ù† ØµØ­ØªÙƒÙ… ÙˆØµØ­Ø© Ø¹Ø§Ø¦Ù„ØªÙƒÙ…. ÙˆÙ‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø§Ø´ ÙŠÙƒÙˆÙ† Ø¹Ù†Ø¯ÙƒÙ… Ø­Ø³Ø§Ø¨ Ø´Ø®ØµÙŠ Ù…Ø³ØªÙ‚Ù„ Ø¹Ù„Ù‰ Ø­Ø³Ø§Ø¨ÙƒÙ… Ø§Ù„Ù…Ù‡Ù†ÙŠ\"\n",
        "text3=\"Ù…Ù† 15 ØºØ´Øª Ø¥Ù„Ù‰ 18 Ø³Ø¨ØªÙ…Ø¨Ø± 2022ØŒ Ø§Ø¯ÙØ¹ÙˆØ§ Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ø¨Ø¯ÙˆÙ† ØªÙ…Ø§Ø³ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø·Ø§Ù‚Ø© Visa Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒÙ… ÙÙŠ Ø§Ù„Ù…ØªØ§Ø¬Ø± MaÃ¢rif Culture , GO Sport Maroc , City Sport Maroc , PUMA Maroc Courir Maroc : ÙˆDaha Fazla Ø§Ø³ØªÙÙŠØ¯ÙˆØ§ Ù…Ù† ØªØ¹ÙˆÙŠØ¶ Ùª20 ğŸ˜ !Ù„Ù„Ø¹Ù„Ù… Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¯ÙØ¹ Ø¨Ø¯ÙˆÙ† ØªÙ…Ø§Ø³ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© Ø¨Ø·Ø§Ù‚Ø© Visa ØªØ³Ù…Ø­ Ù„ÙƒÙ… Ø¨Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ ÙÙŠ Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„ÙƒØ¨ÙŠØ± ğŸ‰ Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ÙÙˆØ² Ø¨Ù‡Ø¯Ø§ÙŠØ§ Ø±Ø§Ø¦Ø¹Ø© ğŸ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø³ÙŠØ§Ø±Ø© !Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ§Ù„ÙŠ : https://bit.ly/3pf9ARC \"\n",
        "text4=\"Ø¨Ù…Ù†Ø§Ø³Ø¨Ø© Ø§Ù„Ø°ÙƒØ±Ù‰ 43 Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØ§Ø¯ÙŠ Ø§Ù„Ø°Ù‡Ø¨ØŒ ÙŠØªÙ‚Ø¯Ù… Ø§Ù„Ù‚Ø±Ø¶ Ø§Ù„ÙÙ„Ø§Ø­ÙŠ Ù„Ù„Ù…ØºØ±Ø¨ Ø¨Ø£Ø­Ø± Ø§Ù„ØªÙ‡Ø§Ù†ÙŠ Ù„ÙƒØ§ÙØ© Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù…ØºØ±Ø¨ÙŠ\"\n",
        "text5=\"Ø¨Ù…Ù†Ø§Ø³Ø¨Ø© Ø§Ù„ÙŠÙˆÙ… Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ø¬Ø§Ù„ÙŠØ© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ø¨Ø§Ù„Ø®Ø§Ø±Ø¬ØŒ Ø§Ù„Ù‚Ø±Ø¶ Ø§Ù„ÙÙ„Ø§Ø­ÙŠ Ù„Ù„Ù…ØºØ±Ø¨ ÙŠØ±Ø­Ø¨ Ø¨ÙƒÙ…. ÙˆÙ„Ù…Ø±Ø§ÙÙ‚ØªÙƒÙ… Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ØŒ Ø§Ù„Ù‚Ø±Ø¶ Ø§Ù„ÙÙ„Ø§Ø­ÙŠ Ù„Ù„Ù…ØºØ±Ø¨ ØµÙ…Ù‘Ù… Ù…Ù† Ø§Ø¬Ù„ÙƒÙ… Air Pack MDM #RESPIRE Ø¹Ø±Ø¶ Ø¨Ù†ÙƒÙŠ Ø´Ø§Ù…Ù„ ÙŠÙ„Ø§Ø¦Ù… Ù…ØªØ·Ù„Ø¨Ø§ØªÙƒÙ… Ùˆ ÙŠÙØªØ­ Ø£Ù…Ø§Ù…ÙƒÙ… Ø£Ø¨ÙˆØ§Ø¨ Ø¹Ø§Ù„Ù… Ù…Ù† Ø§Ù„Ø§Ù…ØªÙŠØ§Ø²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© . \"\n",
        "outputs = arabic_keyword_extract_CV (text)\n",
        "print (outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXr-3jHszIj-",
        "outputId": "5cc07382-cc0b-46fd-c601-6617a7a1147c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'original_text': 'Ø¨Ù…Ù†Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ø¬Ø§Ù„ÙŠØ© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ø¨Ø§Ù„Ø®Ø§Ø±Ø¬ Ø§Ù„Ù‚Ø±Ø¶ Ø§Ù„ÙÙ„Ø§Ø­ÙŠ Ù„Ù„Ù…ØºØ±Ø¨ ÙŠØ±Ø­Ø¨ Ø¨ÙƒÙ… . ÙˆÙ„Ù…Ø±Ø§ÙÙ‚ØªÙƒÙ… Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ø§Ù„Ù‚Ø±Ø¶ Ø§Ù„ÙÙ„Ø§Ø­ÙŠ Ù„Ù„Ù…ØºØ±Ø¨ ØµÙ…Ù… Ø§Ø¬Ù„ÙƒÙ… Air Pack MDM # RESPIRE Ø¹Ø±Ø¶ Ø¨Ù†ÙƒÙŠ Ø´Ø§Ù…Ù„ ÙŠÙ„Ø§Ø¦Ù… Ù…ØªØ·Ù„Ø¨Ø§ØªÙƒÙ… ÙŠÙØªØ­ Ø£Ù…Ø§Ù…ÙƒÙ… Ø£Ø¨ÙˆØ§Ø¨ Ø¹Ø§Ù„Ù… Ø§Ù„Ø§Ù…ØªÙŠØ§Ø²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© .', 'keywords': ['Ø§Ù„Ù‚Ø±Ø¶ Ø§Ù„ÙÙ„Ø§Ø­ÙŠ Ù„Ù„Ù…ØºØ±Ø¨,Ø§Ù„Ù‚Ø±Ø¶ Ø§Ù„ÙÙ„Ø§Ø­ÙŠ Ù„Ù„Ù…ØºØ±Ø¨,Ø§Ù„ÙˆØ·Ù†ÙŠ']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 2- *KeyphraseCountVectorizer*"
      ],
      "metadata": {
        "id": "8NzXIUFey7S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#==================================================\n",
        "# 2- using KeyphraseCountVectorizer\n",
        "#==================================================\n",
        "\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "\n",
        "def arabic_keyword_extract_KCV(text):\n",
        "\n",
        "  import re\n",
        "  text = re.sub('([@A-Za-z0-9_Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€]+)|[^\\w\\s]|#|http\\S+', '', text)\n",
        "  #pos_pattern regexp to ensure extracting keywords which start with nouns\n",
        "  vectorizer = KeyphraseCountVectorizer()\n",
        "  #try:\n",
        "  keywords = []\n",
        "  kw_extractor = KeyBERT(\"paraphrase-multilingual-mpnet-base-v2\")\n",
        "  keywords = kw_extractor.extract_keywords(text , keyphrase_ngram_range=(2,3), vectorizer=vectorizer, stop_words=\"arabic\", top_n=3, use_mmr=True, diversity=0.7)\n",
        "  keywords = [ x[0] for x in keywords]\n",
        "  output = dict()\n",
        "  output[\"original_text\"] = text\n",
        "  output[\"keywords\"] = [','.join(keywords)]\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(output)\n",
        "  df.to_csv(\"arabic_keywords_KCV.csv\")\n",
        "  return output"
      ],
      "metadata": {
        "id": "OD0BtwdIxju2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#testing with 5 posts:\n",
        "\n",
        "outputs= arabic_keyword_extract_KCV(text)\n",
        "print(outputs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfdwMqr1ytti",
        "outputId": "d8e615a1-450e-4c50-d4f3-094eb15eb2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'original_text': 'Ù„ØªÙƒÙ† Ø¹Ø¨Ø±Ø© Ù„Ù„Ø§Ø®Ø±ÙŠÙ† Ø§ØµØ¨Ø­Ù†Ø§ Ù†Ø±Ù‰ Ø§Ù†ØªØ´Ø§Ø± ÙˆØ§Ø³Ø¹ Ù„Ù„ØªØ§ÙÙ‡ÙŠÙ† Ø¨Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆØ§ØµØ¨Ø­Ùˆ Ø­Ø¯ÙŠØ« Ø§Ù„ÙƒÙ„ Ø±ØºÙ… Ø§Ù†Ù‡Ù… Ù„Ù… ÙŠÙ‚Ø¯Ù…Ùˆ Ø§ÙŠ Ù‚ÙŠÙ…Ø© Ù…Ø¶Ø§ÙØ© Ù„Ù„Ù…Ø¬ØªÙ…Ø¹ Ù„Ø§ÙƒÙ† Ù„Ù„Ø§Ø³Ù ÙØ§Ù† Ø§Ù„Ù†Ø¬ÙˆÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙŠØ³ØªØ­Ù‚ÙˆÙ† Ø§Ù† ÙŠØªØµØ¯Ø±Ùˆ Ø§Ù„Ø·ÙˆÙ†Ø¯ÙˆÙ†Ø³ Ù†ÙƒØ§Ø¯ Ù„Ø§Ù†Ø³Ù…Ø¹ Ø¹Ù†Ù‡Ù… Ø´ÙŠØ¡ Ø±ØºÙ… Ø§Ù†Ù‡Ù… ÙŠØ­ØªÙ„ÙˆÙ†Ø© Ù…ÙƒØ§Ù†Ø© Ø±ÙÙŠØ¹Ø© ÙÙŠ Ø§ÙƒØ¨Ø± Ø§Ù„Ù…Ø¤Ø³Ø³Ø§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø§Ø±ÙˆØ¨ÙŠØ© ÙˆØ§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ', 'keywords': ['Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆØ§ØµØ¨Ø­Ùˆ Ø­Ø¯ÙŠØ« Ø§Ù„ÙƒÙ„ Ø±ØºÙ… Ø§Ù†Ù‡Ù…,Ø§ØµØ¨Ø­Ù†Ø§ Ù†Ø±Ù‰ Ø§Ù†ØªØ´Ø§Ø± ÙˆØ§Ø³Ø¹ Ù„Ù„ØªØ§ÙÙ‡ÙŠÙ†,Ù„ØªÙƒÙ† Ø¹Ø¨Ø±Ø©']}\n"
          ]
        }
      ]
    }
  ]
}